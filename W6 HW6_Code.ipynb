{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (ii) Consider a 1-layer NN obtained by removing one of the hidden layers from the 2-layer NN above. Suppose the true data generating process is y = σ(x), where x ∼ N(0, 1). Generate n = 1, 000, 000 data points and fit both NNs by minimizing the average squared loss (you need not use backpropagation here; use scipy.optimize.minimize). Report training errors and optimized weights. Explain why in this case adding another layer increases the training error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The 2-layer NN has more parameters and is therefore more complex than the 1-layer NN. In a complex model, there is a higher risk of overfitting, especially if the true underlying process is simple. Also, The additional layer increases the model's capacity, which means it can represent more complex functions. However, if the true function is simple, the extra capacity isn't necessary and can lead to a model that doesn't generalize well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "from scipy.stats import norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)  # For reproducibility\n",
    "n = 1000000\n",
    "x_data = norm.rvs(size=n)\n",
    "y_data = sigmoid(x_data)  # True data generating process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the 1-layer neural network function \n",
    "def nn_1_layer(weights, inputs):\n",
    "    h1 = sigmoid(weights[0] * inputs)\n",
    "    return weights[1] * h1\n",
    "\n",
    "# Define the 2-layer neural network function\n",
    "def nn_2_layer(weights, inputs):\n",
    "    h1 = sigmoid(weights[0] * inputs)\n",
    "    h2 = sigmoid(weights[1] * h1)\n",
    "    return weights[2] * h2\n",
    "\n",
    "# Define the loss function(MSE)\n",
    "def loss(weights, inputs, true_outputs, nn_function):\n",
    "    predictions = nn_function(weights, inputs)\n",
    "    return np.mean((predictions - true_outputs) ** 2)\n",
    "\n",
    "# Initial guess for weights\n",
    "initial_weights = np.array([0.1, 0.1, 0.1])\n",
    "\n",
    "# Train the 1-layer NN\n",
    "res_1_layer = minimize(fun=loss, \n",
    "                       x0=initial_weights[:2], \n",
    "                       args=(x_data, y_data, nn_1_layer), method='BFGS')\n",
    "trained_weights_1_layer = res_1_layer.x\n",
    "training_error_1_layer = res_1_layer.fun\n",
    "\n",
    "# Train the 2-layer NN\n",
    "res_2_layer = minimize(fun=loss, \n",
    "                       x0=initial_weights, \n",
    "                       args=(x_data, y_data, nn_2_layer), method='BFGS')\n",
    "trained_weights_2_layer = res_2_layer.x\n",
    "training_error_2_layer = res_2_layer.fun\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  message: Optimization terminated successfully.\n",
       "  success: True\n",
       "   status: 0\n",
       "      fun: 5.895695734947004e-10\n",
       "        x: [ 9.998e-01  1.000e+00]\n",
       "      nit: 9\n",
       "      jac: [-7.026e-06 -9.299e-07]\n",
       " hess_inv: [[ 2.344e+01 -2.465e+00]\n",
       "            [-2.465e+00  1.859e+00]]\n",
       "     nfev: 30\n",
       "     njev: 10"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_1_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  message: Optimization terminated successfully.\n",
       "  success: True\n",
       "   status: 0\n",
       "      fun: 0.010075646133647132\n",
       "        x: [ 5.593e+00  3.461e+00  6.731e-01]\n",
       "      nit: 28\n",
       "      jac: [-1.930e-07  4.368e-07  1.510e-06]\n",
       " hess_inv: [[ 6.460e+03  9.728e+02 -6.490e+00]\n",
       "            [ 9.728e+02  1.332e+03 -3.151e+01]\n",
       "            [-6.490e+00 -3.151e+01  1.590e+00]]\n",
       "     nfev: 136\n",
       "     njev: 34"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_2_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training error for 1-layer NN: 5.895695734947004e-10\n",
      "Training error for 2-layer NN: 0.010075646133647132\n",
      "Difference in training error: -0.010075645544077558\n",
      "Optimized weights for 1-layer NN: [0.99982998 1.00001655]\n",
      "Optimized weights for 2-layer NN: [5.59337314 3.46053203 0.67314477]\n"
     ]
    }
   ],
   "source": [
    "print('Training error for 1-layer NN:', training_error_1_layer)\n",
    "print('Training error for 2-layer NN:', training_error_2_layer)\n",
    "print('Difference in training error:', training_error_1_layer - training_error_2_layer)\n",
    "print('Optimized weights for 1-layer NN:', trained_weights_1_layer)\n",
    "print('Optimized weights for 2-layer NN:', trained_weights_2_layer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2\n",
    "\n",
    "### Use the dataset card transdata.csv from the previous homework and maintain the same traintest split.\n",
    "### Fit a feedforward neural network with two ReLU layers using stochastic gradient descent (SGD). Follow this tutorial. Experiment with the number of neurons per layer, the number of epochs, the learning rate for SGD, and the batch size for backpropagation. Report accuracy and F1 score on the test sample. Does your model perform better than a simple decision tree from the last homework?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### After the experiment with the number of neurons per layer, the number of epochs, the learning rate for SGD, and the batch size for backpropagation, it's shown that the accuracy and F1 score on the test sample are lower than the ones for a simple decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000000 entries, 0 to 999999\n",
      "Data columns (total 8 columns):\n",
      " #   Column                          Non-Null Count    Dtype  \n",
      "---  ------                          --------------    -----  \n",
      " 0   distance_from_home              1000000 non-null  float64\n",
      " 1   distance_from_last_transaction  1000000 non-null  float64\n",
      " 2   ratio_to_median_purchase_price  1000000 non-null  float64\n",
      " 3   repeat_retailer                 1000000 non-null  int64  \n",
      " 4   used_chip                       1000000 non-null  int64  \n",
      " 5   used_pin_number                 1000000 non-null  int64  \n",
      " 6   online_order                    1000000 non-null  int64  \n",
      " 7   fraud                           1000000 non-null  int64  \n",
      "dtypes: float64(3), int64(5)\n",
      "memory usage: 61.0 MB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>distance_from_home</th>\n",
       "      <th>distance_from_last_transaction</th>\n",
       "      <th>ratio_to_median_purchase_price</th>\n",
       "      <th>repeat_retailer</th>\n",
       "      <th>used_chip</th>\n",
       "      <th>used_pin_number</th>\n",
       "      <th>online_order</th>\n",
       "      <th>fraud</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>57.877857</td>\n",
       "      <td>0.311140</td>\n",
       "      <td>1.945940</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10.829943</td>\n",
       "      <td>0.175592</td>\n",
       "      <td>1.294219</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.091079</td>\n",
       "      <td>0.805153</td>\n",
       "      <td>0.427715</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.247564</td>\n",
       "      <td>5.600044</td>\n",
       "      <td>0.362663</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>44.190936</td>\n",
       "      <td>0.566486</td>\n",
       "      <td>2.222767</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   distance_from_home  distance_from_last_transaction  \\\n",
       "0           57.877857                        0.311140   \n",
       "1           10.829943                        0.175592   \n",
       "2            5.091079                        0.805153   \n",
       "3            2.247564                        5.600044   \n",
       "4           44.190936                        0.566486   \n",
       "\n",
       "   ratio_to_median_purchase_price  repeat_retailer  used_chip  \\\n",
       "0                        1.945940                1          1   \n",
       "1                        1.294219                1          0   \n",
       "2                        0.427715                1          0   \n",
       "3                        0.362663                1          1   \n",
       "4                        2.222767                1          1   \n",
       "\n",
       "   used_pin_number  online_order  fraud  \n",
       "0                0             0      0  \n",
       "1                0             0      0  \n",
       "2                0             1      0  \n",
       "3                0             1      0  \n",
       "4                0             1      0  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('W5_card_transdata-1.csv')\n",
    "df.info()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "train_size = 500000\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X = df.drop('fraud', axis=1)\n",
    "X = scaler.fit_transform(X)\n",
    "y = df['fraud'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = X[:train_size], X[train_size:], y[:train_size], y[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "#--- Define the hyperparameters\n",
    "learning_rate = 1e-3\n",
    "batch_size = 64\n",
    "epochs = 10\n",
    "#---------------------------\n",
    "\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, X, Y):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)  # Convert X to a PyTorch tensor\n",
    "        self.Y = torch.tensor(Y, dtype=torch.long)  # Convert Y to a PyTorch tensor\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.Y[idx]\n",
    "    \n",
    "\n",
    "train_data = CustomDataset(X_train, y_train)\n",
    "test_data = CustomDataset(X_test, y_test)\n",
    "train_dataloader = DataLoader(train_data, batch_size=64)\n",
    "test_dataloader = DataLoader(test_data, batch_size=64)\n",
    "\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(7, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 2),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "model = NeuralNetwork()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the loss function\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    # Set the model to training mode - important for batch normalization and dropout layers\n",
    "    # Unnecessary in this situation but added for best practices\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * batch_size + len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    # Set the model to evaluation mode - important for batch normalization and dropout layers\n",
    "    # Unnecessary in this situation but added for best practices\n",
    "    model.eval()\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    # Evaluating the model with torch.no_grad() ensures that no gradients are computed during test mode\n",
    "    # also serves to reduce unnecessary gradient computations and memory usage for tensors with requires_grad=True\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 0.674639  [   64/500000]\n",
      "loss: 0.428592  [ 6464/500000]\n",
      "loss: 0.358829  [12864/500000]\n",
      "loss: 0.299000  [19264/500000]\n",
      "loss: 0.231646  [25664/500000]\n",
      "loss: 0.289466  [32064/500000]\n",
      "loss: 0.254577  [38464/500000]\n",
      "loss: 0.339768  [44864/500000]\n",
      "loss: 0.212799  [51264/500000]\n",
      "loss: 0.242656  [57664/500000]\n",
      "loss: 0.173129  [64064/500000]\n",
      "loss: 0.180806  [70464/500000]\n",
      "loss: 0.228233  [76864/500000]\n",
      "loss: 0.183010  [83264/500000]\n",
      "loss: 0.264446  [89664/500000]\n",
      "loss: 0.217681  [96064/500000]\n",
      "loss: 0.098201  [102464/500000]\n",
      "loss: 0.169662  [108864/500000]\n",
      "loss: 0.153682  [115264/500000]\n",
      "loss: 0.159917  [121664/500000]\n",
      "loss: 0.228816  [128064/500000]\n",
      "loss: 0.134855  [134464/500000]\n",
      "loss: 0.210606  [140864/500000]\n",
      "loss: 0.195117  [147264/500000]\n",
      "loss: 0.119279  [153664/500000]\n",
      "loss: 0.127207  [160064/500000]\n",
      "loss: 0.113220  [166464/500000]\n",
      "loss: 0.103192  [172864/500000]\n",
      "loss: 0.120505  [179264/500000]\n",
      "loss: 0.104364  [185664/500000]\n",
      "loss: 0.104943  [192064/500000]\n",
      "loss: 0.066061  [198464/500000]\n",
      "loss: 0.091883  [204864/500000]\n",
      "loss: 0.113922  [211264/500000]\n",
      "loss: 0.101537  [217664/500000]\n",
      "loss: 0.108551  [224064/500000]\n",
      "loss: 0.145253  [230464/500000]\n",
      "loss: 0.158209  [236864/500000]\n",
      "loss: 0.060063  [243264/500000]\n",
      "loss: 0.152844  [249664/500000]\n",
      "loss: 0.139130  [256064/500000]\n",
      "loss: 0.067425  [262464/500000]\n",
      "loss: 0.088713  [268864/500000]\n",
      "loss: 0.136952  [275264/500000]\n",
      "loss: 0.088016  [281664/500000]\n",
      "loss: 0.107396  [288064/500000]\n",
      "loss: 0.127052  [294464/500000]\n",
      "loss: 0.119838  [300864/500000]\n",
      "loss: 0.044922  [307264/500000]\n",
      "loss: 0.115501  [313664/500000]\n",
      "loss: 0.112542  [320064/500000]\n",
      "loss: 0.074899  [326464/500000]\n",
      "loss: 0.074537  [332864/500000]\n",
      "loss: 0.306831  [339264/500000]\n",
      "loss: 0.100471  [345664/500000]\n",
      "loss: 0.084458  [352064/500000]\n",
      "loss: 0.139677  [358464/500000]\n",
      "loss: 0.100753  [364864/500000]\n",
      "loss: 0.075622  [371264/500000]\n",
      "loss: 0.086128  [377664/500000]\n",
      "loss: 0.133716  [384064/500000]\n",
      "loss: 0.122564  [390464/500000]\n",
      "loss: 0.114175  [396864/500000]\n",
      "loss: 0.080006  [403264/500000]\n",
      "loss: 0.116314  [409664/500000]\n",
      "loss: 0.099680  [416064/500000]\n",
      "loss: 0.098813  [422464/500000]\n",
      "loss: 0.087020  [428864/500000]\n",
      "loss: 0.076939  [435264/500000]\n",
      "loss: 0.094226  [441664/500000]\n",
      "loss: 0.139207  [448064/500000]\n",
      "loss: 0.081213  [454464/500000]\n",
      "loss: 0.074758  [460864/500000]\n",
      "loss: 0.038608  [467264/500000]\n",
      "loss: 0.106405  [473664/500000]\n",
      "loss: 0.107794  [480064/500000]\n",
      "loss: 0.084408  [486464/500000]\n",
      "loss: 0.094160  [492864/500000]\n",
      "loss: 0.071905  [499264/500000]\n",
      "Test Error: \n",
      " Accuracy: 96.7%, Avg loss: 0.086681 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.098842  [   64/500000]\n",
      "loss: 0.043806  [ 6464/500000]\n",
      "loss: 0.077934  [12864/500000]\n",
      "loss: 0.051074  [19264/500000]\n",
      "loss: 0.030789  [25664/500000]\n",
      "loss: 0.062172  [32064/500000]\n",
      "loss: 0.087607  [38464/500000]\n",
      "loss: 0.077711  [44864/500000]\n",
      "loss: 0.089334  [51264/500000]\n",
      "loss: 0.083045  [57664/500000]\n",
      "loss: 0.060419  [64064/500000]\n",
      "loss: 0.083399  [70464/500000]\n",
      "loss: 0.081308  [76864/500000]\n",
      "loss: 0.074682  [83264/500000]\n",
      "loss: 0.152696  [89664/500000]\n",
      "loss: 0.201437  [96064/500000]\n",
      "loss: 0.026875  [102464/500000]\n",
      "loss: 0.089933  [108864/500000]\n",
      "loss: 0.059768  [115264/500000]\n",
      "loss: 0.086912  [121664/500000]\n",
      "loss: 0.124655  [128064/500000]\n",
      "loss: 0.049276  [134464/500000]\n",
      "loss: 0.094777  [140864/500000]\n",
      "loss: 0.090536  [147264/500000]\n",
      "loss: 0.057709  [153664/500000]\n",
      "loss: 0.077794  [160064/500000]\n",
      "loss: 0.060128  [166464/500000]\n",
      "loss: 0.047491  [172864/500000]\n",
      "loss: 0.068219  [179264/500000]\n",
      "loss: 0.042911  [185664/500000]\n",
      "loss: 0.035784  [192064/500000]\n",
      "loss: 0.021795  [198464/500000]\n",
      "loss: 0.052809  [204864/500000]\n",
      "loss: 0.053313  [211264/500000]\n",
      "loss: 0.052392  [217664/500000]\n",
      "loss: 0.045468  [224064/500000]\n",
      "loss: 0.081473  [230464/500000]\n",
      "loss: 0.105374  [236864/500000]\n",
      "loss: 0.028288  [243264/500000]\n",
      "loss: 0.091727  [249664/500000]\n",
      "loss: 0.085829  [256064/500000]\n",
      "loss: 0.032045  [262464/500000]\n",
      "loss: 0.036768  [268864/500000]\n",
      "loss: 0.066468  [275264/500000]\n",
      "loss: 0.059364  [281664/500000]\n",
      "loss: 0.081340  [288064/500000]\n",
      "loss: 0.059968  [294464/500000]\n",
      "loss: 0.062406  [300864/500000]\n",
      "loss: 0.039643  [307264/500000]\n",
      "loss: 0.091837  [313664/500000]\n",
      "loss: 0.066306  [320064/500000]\n",
      "loss: 0.037931  [326464/500000]\n",
      "loss: 0.033027  [332864/500000]\n",
      "loss: 0.184710  [339264/500000]\n",
      "loss: 0.064937  [345664/500000]\n",
      "loss: 0.060628  [352064/500000]\n",
      "loss: 0.080249  [358464/500000]\n",
      "loss: 0.055271  [364864/500000]\n",
      "loss: 0.037746  [371264/500000]\n",
      "loss: 0.058282  [377664/500000]\n",
      "loss: 0.097059  [384064/500000]\n",
      "loss: 0.085248  [390464/500000]\n",
      "loss: 0.070788  [396864/500000]\n",
      "loss: 0.056365  [403264/500000]\n",
      "loss: 0.070103  [409664/500000]\n",
      "loss: 0.067243  [416064/500000]\n",
      "loss: 0.064313  [422464/500000]\n",
      "loss: 0.073031  [428864/500000]\n",
      "loss: 0.046641  [435264/500000]\n",
      "loss: 0.070623  [441664/500000]\n",
      "loss: 0.110466  [448064/500000]\n",
      "loss: 0.057873  [454464/500000]\n",
      "loss: 0.048192  [460864/500000]\n",
      "loss: 0.034416  [467264/500000]\n",
      "loss: 0.080019  [473664/500000]\n",
      "loss: 0.078857  [480064/500000]\n",
      "loss: 0.050138  [486464/500000]\n",
      "loss: 0.070150  [492864/500000]\n",
      "loss: 0.046368  [499264/500000]\n",
      "Test Error: \n",
      " Accuracy: 97.8%, Avg loss: 0.060810 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.058714  [   64/500000]\n",
      "loss: 0.027089  [ 6464/500000]\n",
      "loss: 0.063544  [12864/500000]\n",
      "loss: 0.027842  [19264/500000]\n",
      "loss: 0.022287  [25664/500000]\n",
      "loss: 0.035215  [32064/500000]\n",
      "loss: 0.058267  [38464/500000]\n",
      "loss: 0.065246  [44864/500000]\n",
      "loss: 0.078419  [51264/500000]\n",
      "loss: 0.057374  [57664/500000]\n",
      "loss: 0.045128  [64064/500000]\n",
      "loss: 0.047824  [70464/500000]\n",
      "loss: 0.052292  [76864/500000]\n",
      "loss: 0.061721  [83264/500000]\n",
      "loss: 0.138266  [89664/500000]\n",
      "loss: 0.122509  [96064/500000]\n",
      "loss: 0.019187  [102464/500000]\n",
      "loss: 0.084974  [108864/500000]\n",
      "loss: 0.041322  [115264/500000]\n",
      "loss: 0.072819  [121664/500000]\n",
      "loss: 0.095125  [128064/500000]\n",
      "loss: 0.031526  [134464/500000]\n",
      "loss: 0.066728  [140864/500000]\n",
      "loss: 0.064812  [147264/500000]\n",
      "loss: 0.042640  [153664/500000]\n",
      "loss: 0.068944  [160064/500000]\n",
      "loss: 0.050803  [166464/500000]\n",
      "loss: 0.038104  [172864/500000]\n",
      "loss: 0.048509  [179264/500000]\n",
      "loss: 0.029477  [185664/500000]\n",
      "loss: 0.019612  [192064/500000]\n",
      "loss: 0.013734  [198464/500000]\n",
      "loss: 0.045325  [204864/500000]\n",
      "loss: 0.040547  [211264/500000]\n",
      "loss: 0.038302  [217664/500000]\n",
      "loss: 0.029618  [224064/500000]\n",
      "loss: 0.064107  [230464/500000]\n",
      "loss: 0.086094  [236864/500000]\n",
      "loss: 0.018947  [243264/500000]\n",
      "loss: 0.074405  [249664/500000]\n",
      "loss: 0.072896  [256064/500000]\n",
      "loss: 0.022651  [262464/500000]\n",
      "loss: 0.020808  [268864/500000]\n",
      "loss: 0.044872  [275264/500000]\n",
      "loss: 0.052921  [281664/500000]\n",
      "loss: 0.067152  [288064/500000]\n",
      "loss: 0.039069  [294464/500000]\n",
      "loss: 0.045228  [300864/500000]\n",
      "loss: 0.036526  [307264/500000]\n",
      "loss: 0.084896  [313664/500000]\n",
      "loss: 0.042266  [320064/500000]\n",
      "loss: 0.026036  [326464/500000]\n",
      "loss: 0.019609  [332864/500000]\n",
      "loss: 0.124544  [339264/500000]\n",
      "loss: 0.048936  [345664/500000]\n",
      "loss: 0.051688  [352064/500000]\n",
      "loss: 0.062536  [358464/500000]\n",
      "loss: 0.039546  [364864/500000]\n",
      "loss: 0.024810  [371264/500000]\n",
      "loss: 0.047761  [377664/500000]\n",
      "loss: 0.085321  [384064/500000]\n",
      "loss: 0.068597  [390464/500000]\n",
      "loss: 0.053766  [396864/500000]\n",
      "loss: 0.046023  [403264/500000]\n",
      "loss: 0.051945  [409664/500000]\n",
      "loss: 0.052010  [416064/500000]\n",
      "loss: 0.044883  [422464/500000]\n",
      "loss: 0.068834  [428864/500000]\n",
      "loss: 0.034353  [435264/500000]\n",
      "loss: 0.060696  [441664/500000]\n",
      "loss: 0.097208  [448064/500000]\n",
      "loss: 0.047770  [454464/500000]\n",
      "loss: 0.039183  [460864/500000]\n",
      "loss: 0.032566  [467264/500000]\n",
      "loss: 0.070367  [473664/500000]\n",
      "loss: 0.065972  [480064/500000]\n",
      "loss: 0.036892  [486464/500000]\n",
      "loss: 0.059789  [492864/500000]\n",
      "loss: 0.035983  [499264/500000]\n",
      "Test Error: \n",
      " Accuracy: 98.2%, Avg loss: 0.050302 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.043367  [   64/500000]\n",
      "loss: 0.020990  [ 6464/500000]\n",
      "loss: 0.058170  [12864/500000]\n",
      "loss: 0.018310  [19264/500000]\n",
      "loss: 0.018594  [25664/500000]\n",
      "loss: 0.023979  [32064/500000]\n",
      "loss: 0.044318  [38464/500000]\n",
      "loss: 0.062526  [44864/500000]\n",
      "loss: 0.068199  [51264/500000]\n",
      "loss: 0.044883  [57664/500000]\n",
      "loss: 0.038272  [64064/500000]\n",
      "loss: 0.038735  [70464/500000]\n",
      "loss: 0.038385  [76864/500000]\n",
      "loss: 0.057295  [83264/500000]\n",
      "loss: 0.131756  [89664/500000]\n",
      "loss: 0.084822  [96064/500000]\n",
      "loss: 0.016096  [102464/500000]\n",
      "loss: 0.084686  [108864/500000]\n",
      "loss: 0.032871  [115264/500000]\n",
      "loss: 0.066509  [121664/500000]\n",
      "loss: 0.084402  [128064/500000]\n",
      "loss: 0.024399  [134464/500000]\n",
      "loss: 0.053327  [140864/500000]\n",
      "loss: 0.054374  [147264/500000]\n",
      "loss: 0.036195  [153664/500000]\n",
      "loss: 0.063993  [160064/500000]\n",
      "loss: 0.046420  [166464/500000]\n",
      "loss: 0.035191  [172864/500000]\n",
      "loss: 0.039682  [179264/500000]\n",
      "loss: 0.024001  [185664/500000]\n",
      "loss: 0.012720  [192064/500000]\n",
      "loss: 0.010295  [198464/500000]\n",
      "loss: 0.039314  [204864/500000]\n",
      "loss: 0.034575  [211264/500000]\n",
      "loss: 0.030686  [217664/500000]\n",
      "loss: 0.022402  [224064/500000]\n",
      "loss: 0.056039  [230464/500000]\n",
      "loss: 0.074519  [236864/500000]\n",
      "loss: 0.014191  [243264/500000]\n",
      "loss: 0.065221  [249664/500000]\n",
      "loss: 0.067074  [256064/500000]\n",
      "loss: 0.017885  [262464/500000]\n",
      "loss: 0.014862  [268864/500000]\n",
      "loss: 0.035218  [275264/500000]\n",
      "loss: 0.050048  [281664/500000]\n",
      "loss: 0.057175  [288064/500000]\n",
      "loss: 0.030426  [294464/500000]\n",
      "loss: 0.037869  [300864/500000]\n",
      "loss: 0.033203  [307264/500000]\n",
      "loss: 0.080086  [313664/500000]\n",
      "loss: 0.029887  [320064/500000]\n",
      "loss: 0.020440  [326464/500000]\n",
      "loss: 0.013595  [332864/500000]\n",
      "loss: 0.097531  [339264/500000]\n",
      "loss: 0.040346  [345664/500000]\n",
      "loss: 0.045590  [352064/500000]\n",
      "loss: 0.054164  [358464/500000]\n",
      "loss: 0.031872  [364864/500000]\n",
      "loss: 0.018643  [371264/500000]\n",
      "loss: 0.041628  [377664/500000]\n",
      "loss: 0.078352  [384064/500000]\n",
      "loss: 0.058246  [390464/500000]\n",
      "loss: 0.045342  [396864/500000]\n",
      "loss: 0.040071  [403264/500000]\n",
      "loss: 0.042859  [409664/500000]\n",
      "loss: 0.042846  [416064/500000]\n",
      "loss: 0.034407  [422464/500000]\n",
      "loss: 0.064712  [428864/500000]\n",
      "loss: 0.027530  [435264/500000]\n",
      "loss: 0.054604  [441664/500000]\n",
      "loss: 0.088330  [448064/500000]\n",
      "loss: 0.041296  [454464/500000]\n",
      "loss: 0.034312  [460864/500000]\n",
      "loss: 0.031183  [467264/500000]\n",
      "loss: 0.065388  [473664/500000]\n",
      "loss: 0.057874  [480064/500000]\n",
      "loss: 0.030294  [486464/500000]\n",
      "loss: 0.053732  [492864/500000]\n",
      "loss: 0.030399  [499264/500000]\n",
      "Test Error: \n",
      " Accuracy: 98.5%, Avg loss: 0.044634 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.035240  [   64/500000]\n",
      "loss: 0.018163  [ 6464/500000]\n",
      "loss: 0.054865  [12864/500000]\n",
      "loss: 0.013696  [19264/500000]\n",
      "loss: 0.016283  [25664/500000]\n",
      "loss: 0.018032  [32064/500000]\n",
      "loss: 0.037015  [38464/500000]\n",
      "loss: 0.059302  [44864/500000]\n",
      "loss: 0.058934  [51264/500000]\n",
      "loss: 0.037928  [57664/500000]\n",
      "loss: 0.034062  [64064/500000]\n",
      "loss: 0.034613  [70464/500000]\n",
      "loss: 0.030259  [76864/500000]\n",
      "loss: 0.054305  [83264/500000]\n",
      "loss: 0.126563  [89664/500000]\n",
      "loss: 0.069346  [96064/500000]\n",
      "loss: 0.014143  [102464/500000]\n",
      "loss: 0.085338  [108864/500000]\n",
      "loss: 0.028166  [115264/500000]\n",
      "loss: 0.062630  [121664/500000]\n",
      "loss: 0.079027  [128064/500000]\n",
      "loss: 0.020729  [134464/500000]\n",
      "loss: 0.046219  [140864/500000]\n",
      "loss: 0.049050  [147264/500000]\n",
      "loss: 0.032377  [153664/500000]\n",
      "loss: 0.059642  [160064/500000]\n",
      "loss: 0.042776  [166464/500000]\n",
      "loss: 0.033803  [172864/500000]\n",
      "loss: 0.035140  [179264/500000]\n",
      "loss: 0.020949  [185664/500000]\n",
      "loss: 0.009154  [192064/500000]\n",
      "loss: 0.008357  [198464/500000]\n",
      "loss: 0.034597  [204864/500000]\n",
      "loss: 0.030860  [211264/500000]\n",
      "loss: 0.025737  [217664/500000]\n",
      "loss: 0.018361  [224064/500000]\n",
      "loss: 0.050689  [230464/500000]\n",
      "loss: 0.066810  [236864/500000]\n",
      "loss: 0.011856  [243264/500000]\n",
      "loss: 0.058511  [249664/500000]\n",
      "loss: 0.062683  [256064/500000]\n",
      "loss: 0.014846  [262464/500000]\n",
      "loss: 0.011650  [268864/500000]\n",
      "loss: 0.030068  [275264/500000]\n",
      "loss: 0.047555  [281664/500000]\n",
      "loss: 0.049812  [288064/500000]\n",
      "loss: 0.025566  [294464/500000]\n",
      "loss: 0.033607  [300864/500000]\n",
      "loss: 0.030314  [307264/500000]\n",
      "loss: 0.075218  [313664/500000]\n",
      "loss: 0.023275  [320064/500000]\n",
      "loss: 0.017164  [326464/500000]\n",
      "loss: 0.010474  [332864/500000]\n",
      "loss: 0.086292  [339264/500000]\n",
      "loss: 0.034912  [345664/500000]\n",
      "loss: 0.040725  [352064/500000]\n",
      "loss: 0.048791  [358464/500000]\n",
      "loss: 0.027418  [364864/500000]\n",
      "loss: 0.015195  [371264/500000]\n",
      "loss: 0.037378  [377664/500000]\n",
      "loss: 0.072686  [384064/500000]\n",
      "loss: 0.050545  [390464/500000]\n",
      "loss: 0.040125  [396864/500000]\n",
      "loss: 0.036236  [403264/500000]\n",
      "loss: 0.037590  [409664/500000]\n",
      "loss: 0.036365  [416064/500000]\n",
      "loss: 0.028266  [422464/500000]\n",
      "loss: 0.060073  [428864/500000]\n",
      "loss: 0.022542  [435264/500000]\n",
      "loss: 0.050118  [441664/500000]\n",
      "loss: 0.081329  [448064/500000]\n",
      "loss: 0.036148  [454464/500000]\n",
      "loss: 0.030985  [460864/500000]\n",
      "loss: 0.029458  [467264/500000]\n",
      "loss: 0.062346  [473664/500000]\n",
      "loss: 0.051632  [480064/500000]\n",
      "loss: 0.026452  [486464/500000]\n",
      "loss: 0.049691  [492864/500000]\n",
      "loss: 0.026335  [499264/500000]\n",
      "Test Error: \n",
      " Accuracy: 98.6%, Avg loss: 0.040799 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.029681  [   64/500000]\n",
      "loss: 0.016200  [ 6464/500000]\n",
      "loss: 0.052525  [12864/500000]\n",
      "loss: 0.011123  [19264/500000]\n",
      "loss: 0.014748  [25664/500000]\n",
      "loss: 0.014421  [32064/500000]\n",
      "loss: 0.032490  [38464/500000]\n",
      "loss: 0.055494  [44864/500000]\n",
      "loss: 0.051047  [51264/500000]\n",
      "loss: 0.033731  [57664/500000]\n",
      "loss: 0.030961  [64064/500000]\n",
      "loss: 0.031715  [70464/500000]\n",
      "loss: 0.025113  [76864/500000]\n",
      "loss: 0.051298  [83264/500000]\n",
      "loss: 0.121512  [89664/500000]\n",
      "loss: 0.061156  [96064/500000]\n",
      "loss: 0.012572  [102464/500000]\n",
      "loss: 0.086083  [108864/500000]\n",
      "loss: 0.025224  [115264/500000]\n",
      "loss: 0.059615  [121664/500000]\n",
      "loss: 0.075261  [128064/500000]\n",
      "loss: 0.018352  [134464/500000]\n",
      "loss: 0.041528  [140864/500000]\n",
      "loss: 0.045718  [147264/500000]\n",
      "loss: 0.029420  [153664/500000]\n",
      "loss: 0.055271  [160064/500000]\n",
      "loss: 0.039310  [166464/500000]\n",
      "loss: 0.032541  [172864/500000]\n",
      "loss: 0.032086  [179264/500000]\n",
      "loss: 0.018835  [185664/500000]\n",
      "loss: 0.007091  [192064/500000]\n",
      "loss: 0.007059  [198464/500000]\n",
      "loss: 0.030822  [204864/500000]\n",
      "loss: 0.027804  [211264/500000]\n",
      "loss: 0.022196  [217664/500000]\n",
      "loss: 0.015739  [224064/500000]\n",
      "loss: 0.046506  [230464/500000]\n",
      "loss: 0.061109  [236864/500000]\n",
      "loss: 0.010617  [243264/500000]\n",
      "loss: 0.053052  [249664/500000]\n",
      "loss: 0.058626  [256064/500000]\n",
      "loss: 0.012666  [262464/500000]\n",
      "loss: 0.009660  [268864/500000]\n",
      "loss: 0.026865  [275264/500000]\n",
      "loss: 0.045522  [281664/500000]\n",
      "loss: 0.044147  [288064/500000]\n",
      "loss: 0.022224  [294464/500000]\n",
      "loss: 0.030528  [300864/500000]\n",
      "loss: 0.027688  [307264/500000]\n",
      "loss: 0.070413  [313664/500000]\n",
      "loss: 0.019304  [320064/500000]\n",
      "loss: 0.014966  [326464/500000]\n",
      "loss: 0.008501  [332864/500000]\n",
      "loss: 0.079820  [339264/500000]\n",
      "loss: 0.031223  [345664/500000]\n",
      "loss: 0.036662  [352064/500000]\n",
      "loss: 0.044740  [358464/500000]\n",
      "loss: 0.024455  [364864/500000]\n",
      "loss: 0.013010  [371264/500000]\n",
      "loss: 0.034121  [377664/500000]\n",
      "loss: 0.067518  [384064/500000]\n",
      "loss: 0.044343  [390464/500000]\n",
      "loss: 0.036159  [396864/500000]\n",
      "loss: 0.033594  [403264/500000]\n",
      "loss: 0.034226  [409664/500000]\n",
      "loss: 0.031338  [416064/500000]\n",
      "loss: 0.024453  [422464/500000]\n",
      "loss: 0.055531  [428864/500000]\n",
      "loss: 0.018440  [435264/500000]\n",
      "loss: 0.046713  [441664/500000]\n",
      "loss: 0.075365  [448064/500000]\n",
      "loss: 0.031908  [454464/500000]\n",
      "loss: 0.028488  [460864/500000]\n",
      "loss: 0.027590  [467264/500000]\n",
      "loss: 0.060123  [473664/500000]\n",
      "loss: 0.046328  [480064/500000]\n",
      "loss: 0.023951  [486464/500000]\n",
      "loss: 0.046550  [492864/500000]\n",
      "loss: 0.023030  [499264/500000]\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 0.037831 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.025292  [   64/500000]\n",
      "loss: 0.014674  [ 6464/500000]\n",
      "loss: 0.050755  [12864/500000]\n",
      "loss: 0.009530  [19264/500000]\n",
      "loss: 0.013612  [25664/500000]\n",
      "loss: 0.011995  [32064/500000]\n",
      "loss: 0.029500  [38464/500000]\n",
      "loss: 0.051747  [44864/500000]\n",
      "loss: 0.044109  [51264/500000]\n",
      "loss: 0.031059  [57664/500000]\n",
      "loss: 0.028221  [64064/500000]\n",
      "loss: 0.029292  [70464/500000]\n",
      "loss: 0.021384  [76864/500000]\n",
      "loss: 0.048370  [83264/500000]\n",
      "loss: 0.116358  [89664/500000]\n",
      "loss: 0.055474  [96064/500000]\n",
      "loss: 0.011045  [102464/500000]\n",
      "loss: 0.086485  [108864/500000]\n",
      "loss: 0.023338  [115264/500000]\n",
      "loss: 0.056804  [121664/500000]\n",
      "loss: 0.072150  [128064/500000]\n",
      "loss: 0.016585  [134464/500000]\n",
      "loss: 0.038207  [140864/500000]\n",
      "loss: 0.042728  [147264/500000]\n",
      "loss: 0.026732  [153664/500000]\n",
      "loss: 0.051132  [160064/500000]\n",
      "loss: 0.035969  [166464/500000]\n",
      "loss: 0.031306  [172864/500000]\n",
      "loss: 0.029595  [179264/500000]\n",
      "loss: 0.017206  [185664/500000]\n",
      "loss: 0.005758  [192064/500000]\n",
      "loss: 0.006099  [198464/500000]\n",
      "loss: 0.027746  [204864/500000]\n",
      "loss: 0.025088  [211264/500000]\n",
      "loss: 0.019438  [217664/500000]\n",
      "loss: 0.013856  [224064/500000]\n",
      "loss: 0.043176  [230464/500000]\n",
      "loss: 0.056576  [236864/500000]\n",
      "loss: 0.009904  [243264/500000]\n",
      "loss: 0.048337  [249664/500000]\n",
      "loss: 0.054615  [256064/500000]\n",
      "loss: 0.010933  [262464/500000]\n",
      "loss: 0.008341  [268864/500000]\n",
      "loss: 0.024572  [275264/500000]\n",
      "loss: 0.043682  [281664/500000]\n",
      "loss: 0.039599  [288064/500000]\n",
      "loss: 0.019726  [294464/500000]\n",
      "loss: 0.028156  [300864/500000]\n",
      "loss: 0.025346  [307264/500000]\n",
      "loss: 0.065498  [313664/500000]\n",
      "loss: 0.016713  [320064/500000]\n",
      "loss: 0.013308  [326464/500000]\n",
      "loss: 0.007117  [332864/500000]\n",
      "loss: 0.075210  [339264/500000]\n",
      "loss: 0.028476  [345664/500000]\n",
      "loss: 0.033186  [352064/500000]\n",
      "loss: 0.041523  [358464/500000]\n",
      "loss: 0.022295  [364864/500000]\n",
      "loss: 0.011471  [371264/500000]\n",
      "loss: 0.031254  [377664/500000]\n",
      "loss: 0.062770  [384064/500000]\n",
      "loss: 0.039255  [390464/500000]\n",
      "loss: 0.032918  [396864/500000]\n",
      "loss: 0.031735  [403264/500000]\n",
      "loss: 0.031880  [409664/500000]\n",
      "loss: 0.027438  [416064/500000]\n",
      "loss: 0.021759  [422464/500000]\n",
      "loss: 0.051449  [428864/500000]\n",
      "loss: 0.015926  [435264/500000]\n",
      "loss: 0.043721  [441664/500000]\n",
      "loss: 0.070368  [448064/500000]\n",
      "loss: 0.028941  [454464/500000]\n",
      "loss: 0.026344  [460864/500000]\n",
      "loss: 0.026308  [467264/500000]\n",
      "loss: 0.057392  [473664/500000]\n",
      "loss: 0.041778  [480064/500000]\n",
      "loss: 0.022056  [486464/500000]\n",
      "loss: 0.043493  [492864/500000]\n",
      "loss: 0.020548  [499264/500000]\n",
      "Test Error: \n",
      " Accuracy: 98.8%, Avg loss: 0.035378 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.022009  [   64/500000]\n",
      "loss: 0.013912  [ 6464/500000]\n",
      "loss: 0.049401  [12864/500000]\n",
      "loss: 0.008539  [19264/500000]\n",
      "loss: 0.012707  [25664/500000]\n",
      "loss: 0.010241  [32064/500000]\n",
      "loss: 0.027193  [38464/500000]\n",
      "loss: 0.048357  [44864/500000]\n",
      "loss: 0.038124  [51264/500000]\n",
      "loss: 0.029205  [57664/500000]\n",
      "loss: 0.025766  [64064/500000]\n",
      "loss: 0.027366  [70464/500000]\n",
      "loss: 0.018706  [76864/500000]\n",
      "loss: 0.045543  [83264/500000]\n",
      "loss: 0.111030  [89664/500000]\n",
      "loss: 0.050901  [96064/500000]\n",
      "loss: 0.009753  [102464/500000]\n",
      "loss: 0.086372  [108864/500000]\n",
      "loss: 0.021965  [115264/500000]\n",
      "loss: 0.054293  [121664/500000]\n",
      "loss: 0.069331  [128064/500000]\n",
      "loss: 0.015225  [134464/500000]\n",
      "loss: 0.035645  [140864/500000]\n",
      "loss: 0.040329  [147264/500000]\n",
      "loss: 0.024527  [153664/500000]\n",
      "loss: 0.047255  [160064/500000]\n",
      "loss: 0.032907  [166464/500000]\n",
      "loss: 0.030150  [172864/500000]\n",
      "loss: 0.027607  [179264/500000]\n",
      "loss: 0.015960  [185664/500000]\n",
      "loss: 0.004814  [192064/500000]\n",
      "loss: 0.005343  [198464/500000]\n",
      "loss: 0.025386  [204864/500000]\n",
      "loss: 0.022824  [211264/500000]\n",
      "loss: 0.017177  [217664/500000]\n",
      "loss: 0.012309  [224064/500000]\n",
      "loss: 0.040401  [230464/500000]\n",
      "loss: 0.052909  [236864/500000]\n",
      "loss: 0.009503  [243264/500000]\n",
      "loss: 0.044318  [249664/500000]\n",
      "loss: 0.050794  [256064/500000]\n",
      "loss: 0.009467  [262464/500000]\n",
      "loss: 0.007380  [268864/500000]\n",
      "loss: 0.022821  [275264/500000]\n",
      "loss: 0.042022  [281664/500000]\n",
      "loss: 0.035687  [288064/500000]\n",
      "loss: 0.017774  [294464/500000]\n",
      "loss: 0.026280  [300864/500000]\n",
      "loss: 0.023136  [307264/500000]\n",
      "loss: 0.060652  [313664/500000]\n",
      "loss: 0.014886  [320064/500000]\n",
      "loss: 0.012006  [326464/500000]\n",
      "loss: 0.006093  [332864/500000]\n",
      "loss: 0.071696  [339264/500000]\n",
      "loss: 0.026306  [345664/500000]\n",
      "loss: 0.030221  [352064/500000]\n",
      "loss: 0.038594  [358464/500000]\n",
      "loss: 0.020610  [364864/500000]\n",
      "loss: 0.010292  [371264/500000]\n",
      "loss: 0.028801  [377664/500000]\n",
      "loss: 0.058366  [384064/500000]\n",
      "loss: 0.034948  [390464/500000]\n",
      "loss: 0.030346  [396864/500000]\n",
      "loss: 0.030270  [403264/500000]\n",
      "loss: 0.030194  [409664/500000]\n",
      "loss: 0.024248  [416064/500000]\n",
      "loss: 0.019817  [422464/500000]\n",
      "loss: 0.047421  [428864/500000]\n",
      "loss: 0.013884  [435264/500000]\n",
      "loss: 0.041177  [441664/500000]\n",
      "loss: 0.065867  [448064/500000]\n",
      "loss: 0.026447  [454464/500000]\n",
      "loss: 0.024463  [460864/500000]\n",
      "loss: 0.025128  [467264/500000]\n",
      "loss: 0.054861  [473664/500000]\n",
      "loss: 0.037763  [480064/500000]\n",
      "loss: 0.020690  [486464/500000]\n",
      "loss: 0.040735  [492864/500000]\n",
      "loss: 0.018523  [499264/500000]\n",
      "Test Error: \n",
      " Accuracy: 98.9%, Avg loss: 0.033257 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.019390  [   64/500000]\n",
      "loss: 0.013381  [ 6464/500000]\n",
      "loss: 0.048044  [12864/500000]\n",
      "loss: 0.007849  [19264/500000]\n",
      "loss: 0.012034  [25664/500000]\n",
      "loss: 0.008894  [32064/500000]\n",
      "loss: 0.025291  [38464/500000]\n",
      "loss: 0.045124  [44864/500000]\n",
      "loss: 0.033279  [51264/500000]\n",
      "loss: 0.027835  [57664/500000]\n",
      "loss: 0.023555  [64064/500000]\n",
      "loss: 0.025769  [70464/500000]\n",
      "loss: 0.016555  [76864/500000]\n",
      "loss: 0.042758  [83264/500000]\n",
      "loss: 0.105692  [89664/500000]\n",
      "loss: 0.046847  [96064/500000]\n",
      "loss: 0.008688  [102464/500000]\n",
      "loss: 0.085844  [108864/500000]\n",
      "loss: 0.020928  [115264/500000]\n",
      "loss: 0.051980  [121664/500000]\n",
      "loss: 0.066608  [128064/500000]\n",
      "loss: 0.014151  [134464/500000]\n",
      "loss: 0.033501  [140864/500000]\n",
      "loss: 0.038394  [147264/500000]\n",
      "loss: 0.022765  [153664/500000]\n",
      "loss: 0.043518  [160064/500000]\n",
      "loss: 0.030005  [166464/500000]\n",
      "loss: 0.028965  [172864/500000]\n",
      "loss: 0.026010  [179264/500000]\n",
      "loss: 0.014925  [185664/500000]\n",
      "loss: 0.004113  [192064/500000]\n",
      "loss: 0.004727  [198464/500000]\n",
      "loss: 0.023341  [204864/500000]\n",
      "loss: 0.020917  [211264/500000]\n",
      "loss: 0.015326  [217664/500000]\n",
      "loss: 0.011032  [224064/500000]\n",
      "loss: 0.037947  [230464/500000]\n",
      "loss: 0.049799  [236864/500000]\n",
      "loss: 0.009281  [243264/500000]\n",
      "loss: 0.040783  [249664/500000]\n",
      "loss: 0.047064  [256064/500000]\n",
      "loss: 0.008263  [262464/500000]\n",
      "loss: 0.006678  [268864/500000]\n",
      "loss: 0.021445  [275264/500000]\n",
      "loss: 0.040552  [281664/500000]\n",
      "loss: 0.032333  [288064/500000]\n",
      "loss: 0.016082  [294464/500000]\n",
      "loss: 0.024776  [300864/500000]\n",
      "loss: 0.020963  [307264/500000]\n",
      "loss: 0.056004  [313664/500000]\n",
      "loss: 0.013508  [320064/500000]\n",
      "loss: 0.011003  [326464/500000]\n",
      "loss: 0.005322  [332864/500000]\n",
      "loss: 0.068659  [339264/500000]\n",
      "loss: 0.024479  [345664/500000]\n",
      "loss: 0.027788  [352064/500000]\n",
      "loss: 0.035772  [358464/500000]\n",
      "loss: 0.019327  [364864/500000]\n",
      "loss: 0.009370  [371264/500000]\n",
      "loss: 0.026660  [377664/500000]\n",
      "loss: 0.054172  [384064/500000]\n",
      "loss: 0.031314  [390464/500000]\n",
      "loss: 0.028140  [396864/500000]\n",
      "loss: 0.029172  [403264/500000]\n",
      "loss: 0.028874  [409664/500000]\n",
      "loss: 0.021615  [416064/500000]\n",
      "loss: 0.018353  [422464/500000]\n",
      "loss: 0.043487  [428864/500000]\n",
      "loss: 0.012206  [435264/500000]\n",
      "loss: 0.038954  [441664/500000]\n",
      "loss: 0.061738  [448064/500000]\n",
      "loss: 0.024403  [454464/500000]\n",
      "loss: 0.022748  [460864/500000]\n",
      "loss: 0.023998  [467264/500000]\n",
      "loss: 0.052395  [473664/500000]\n",
      "loss: 0.034184  [480064/500000]\n",
      "loss: 0.019599  [486464/500000]\n",
      "loss: 0.038224  [492864/500000]\n",
      "loss: 0.016718  [499264/500000]\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.031369 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.017212  [   64/500000]\n",
      "loss: 0.012935  [ 6464/500000]\n",
      "loss: 0.046630  [12864/500000]\n",
      "loss: 0.007364  [19264/500000]\n",
      "loss: 0.011519  [25664/500000]\n",
      "loss: 0.007835  [32064/500000]\n",
      "loss: 0.023706  [38464/500000]\n",
      "loss: 0.042019  [44864/500000]\n",
      "loss: 0.029571  [51264/500000]\n",
      "loss: 0.026802  [57664/500000]\n",
      "loss: 0.021556  [64064/500000]\n",
      "loss: 0.024405  [70464/500000]\n",
      "loss: 0.014795  [76864/500000]\n",
      "loss: 0.039942  [83264/500000]\n",
      "loss: 0.100158  [89664/500000]\n",
      "loss: 0.043168  [96064/500000]\n",
      "loss: 0.007795  [102464/500000]\n",
      "loss: 0.085084  [108864/500000]\n",
      "loss: 0.020125  [115264/500000]\n",
      "loss: 0.049788  [121664/500000]\n",
      "loss: 0.063887  [128064/500000]\n",
      "loss: 0.013291  [134464/500000]\n",
      "loss: 0.031640  [140864/500000]\n",
      "loss: 0.036734  [147264/500000]\n",
      "loss: 0.021281  [153664/500000]\n",
      "loss: 0.039859  [160064/500000]\n",
      "loss: 0.027266  [166464/500000]\n",
      "loss: 0.027734  [172864/500000]\n",
      "loss: 0.024650  [179264/500000]\n",
      "loss: 0.014066  [185664/500000]\n",
      "loss: 0.003576  [192064/500000]\n",
      "loss: 0.004213  [198464/500000]\n",
      "loss: 0.021647  [204864/500000]\n",
      "loss: 0.019205  [211264/500000]\n",
      "loss: 0.013735  [217664/500000]\n",
      "loss: 0.009967  [224064/500000]\n",
      "loss: 0.035731  [230464/500000]\n",
      "loss: 0.047255  [236864/500000]\n",
      "loss: 0.009156  [243264/500000]\n",
      "loss: 0.037608  [249664/500000]\n",
      "loss: 0.043414  [256064/500000]\n",
      "loss: 0.007266  [262464/500000]\n",
      "loss: 0.006143  [268864/500000]\n",
      "loss: 0.020364  [275264/500000]\n",
      "loss: 0.039248  [281664/500000]\n",
      "loss: 0.029433  [288064/500000]\n",
      "loss: 0.014631  [294464/500000]\n",
      "loss: 0.023576  [300864/500000]\n",
      "loss: 0.018865  [307264/500000]\n",
      "loss: 0.051681  [313664/500000]\n",
      "loss: 0.012423  [320064/500000]\n",
      "loss: 0.010202  [326464/500000]\n",
      "loss: 0.004712  [332864/500000]\n",
      "loss: 0.065903  [339264/500000]\n",
      "loss: 0.022875  [345664/500000]\n",
      "loss: 0.025652  [352064/500000]\n",
      "loss: 0.033093  [358464/500000]\n",
      "loss: 0.018351  [364864/500000]\n",
      "loss: 0.008621  [371264/500000]\n",
      "loss: 0.024696  [377664/500000]\n",
      "loss: 0.050246  [384064/500000]\n",
      "loss: 0.028196  [390464/500000]\n",
      "loss: 0.026259  [396864/500000]\n",
      "loss: 0.028367  [403264/500000]\n",
      "loss: 0.027804  [409664/500000]\n",
      "loss: 0.019416  [416064/500000]\n",
      "loss: 0.017152  [422464/500000]\n",
      "loss: 0.039678  [428864/500000]\n",
      "loss: 0.010804  [435264/500000]\n",
      "loss: 0.036983  [441664/500000]\n",
      "loss: 0.057949  [448064/500000]\n",
      "loss: 0.022578  [454464/500000]\n",
      "loss: 0.021172  [460864/500000]\n",
      "loss: 0.022866  [467264/500000]\n",
      "loss: 0.049922  [473664/500000]\n",
      "loss: 0.031027  [480064/500000]\n",
      "loss: 0.018695  [486464/500000]\n",
      "loss: 0.035934  [492864/500000]\n",
      "loss: 0.015051  [499264/500000]\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.029668 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    test_loop(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN Model\n",
      "Testing F1 Score: 0.9440298073004599\n",
      "Testing Accuracy: 0.990386\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "# Calculate the F1 score and accuracy\n",
    "y_pred = model(torch.tensor(X_test, dtype=torch.float32)).argmax(1).numpy()\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Print the F1 scores and accuracies for each epoch\n",
    "print('NN Model')\n",
    "print(\"Testing F1 Score:\", f1)\n",
    "print(\"Testing Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Decision Tree\n",
      "Testing F1: 0.9998284949863367\n",
      "Testing Accuracy: 0.99997\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "clf_original = DecisionTreeClassifier(criterion='entropy', random_state=123)\n",
    "\n",
    "# Fit the classifier to the original training data\n",
    "clf_original.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the original training and testing data\n",
    "y_train_pred_original = clf_original.predict(X_train)\n",
    "y_test_pred_original = clf_original.predict(X_test)\n",
    "\n",
    "# Calculate evaluation metrics for the original training data\n",
    "train_accuracy_original = accuracy_score(y_train, y_train_pred_original)\n",
    "train_f1_original = f1_score(y_train, y_train_pred_original)\n",
    "\n",
    "# Calculate evaluation metrics for the testing data\n",
    "test_accuracy_original = accuracy_score(y_test, y_test_pred_original)\n",
    "test_f1_original = f1_score(y_test, y_test_pred_original)\n",
    "\n",
    "print('Original Decision Tree')\n",
    "print('Testing F1:', test_f1_original)\n",
    "print('Testing Accuracy:', test_accuracy_original)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
